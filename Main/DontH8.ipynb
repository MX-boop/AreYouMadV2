{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import necessary libraries\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_docs as tfdocs\n",
    "from datetime import datetime as dt\n",
    "import keras_tuner as kt\n",
    "\n",
    "print(f\"TensorFlow version:{tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1.5: Set up GPU\n",
    "physical_devices = tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1.6: Load TensorBoard And Kill old logs\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Clear any logs from previous runs\n",
    "os.system('rm -rf ./logs/')\n",
    "\n",
    "clear_previous_training = True\n",
    "if clear_previous_training:\n",
    "    os.system('rm -rf AreYouMad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Define constants and load datasets\n",
    "batch_size = 32\n",
    "seed = 42\n",
    "\n",
    "raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'OrganizedDataToxic', \n",
    "    batch_size=batch_size, \n",
    "    validation_split=0.2, \n",
    "    subset='training', \n",
    "    seed=seed)\n",
    "\n",
    "raw_val_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'OrganizedDataToxic', \n",
    "    batch_size=batch_size, \n",
    "    validation_split=0.2, \n",
    "    subset='validation', \n",
    "    seed=seed)\n",
    "\n",
    "raw_test_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'OrganizedDataToxic', \n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Define custom text standardization function\n",
    "def custom_standardization(input_data):\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "  return tf.strings.regex_replace(stripped_html,\n",
    "                                  '[%s]' % re.escape(string.punctuation),\n",
    "                                  '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Define text vectorization parameters and create vectorization layer\n",
    "max_features = 10000\n",
    "sequence_length = 250\n",
    "\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=max_features,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Adapt the vectorization layer to the training data\n",
    "train_text = raw_train_ds.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Define functions for vectorizing text data\n",
    "def vectorize_text(text, label):\n",
    "  text = tf.expand_dims(text, -1)\n",
    "  return vectorize_layer(text), label\n",
    "\n",
    "train_ds = raw_train_ds.map(vectorize_text)\n",
    "val_ds = raw_val_ds.map(vectorize_text)\n",
    "test_ds = raw_test_ds.map(vectorize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Configure dataset caching and prefetching\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 16\n",
    "\n",
    "def model_builder(hp):\n",
    "\n",
    "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "  hp_units = hp.Int('units', min_value=32, max_value=512, step=32)\n",
    "\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(max_features + 1, embedding_dim),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(hp_units, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid'),])\n",
    "  \n",
    "  model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=hp_learning_rate),\n",
    "                loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "                metrics=['accuracy'])\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.RandomSearch(model_builder,\n",
    "                     objective='val_accuracy',\n",
    "                     max_trials=10,\n",
    "                     directory='AreYouMad',\n",
    "                     project_name='AreYouMad')\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"logs/fit/\" + dt.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the hyperparameter search\n",
    "tuner.search(train_ds, epochs=50, validation_data=val_ds, callbacks=[stop_early, tensorboard_callback])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(tuner.get_best_hyperparameters(num_trials=1)[0].values)\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
    "layer is {best_hps.get('units')} and the optimal learning rate for the optimizer\n",
    "is {best_hps.get('learning_rate')}.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model with the optimal hyperparameters and train it on the data for 50 epochs\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(train_ds, validation_data=val_ds, epochs=50, callbacks=[stop_early, tensorboard_callback])\n",
    "\n",
    "val_acc_per_epoch = history.history['val_accuracy']\n",
    "best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
    "print('Best epoch: %d' % (best_epoch,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypermodel = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# Retrain the model\n",
    "history = hypermodel.fit(train_ds, validation_data=val_ds, epochs=best_epoch, callbacks=[stop_early, tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Plot training and validation loss\n",
    "history_dict = history.history\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'c', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Plot training and validation accuracy\n",
    "plt.plot(epochs, acc, 'c', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Define export model and compile it\n",
    "export_model = tf.keras.Sequential([\n",
    "  vectorize_layer,\n",
    "  hypermodel,\n",
    "  tf.keras.layers.Activation('sigmoid')\n",
    "])\n",
    "\n",
    "export_model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=best_hps.get('learning_rate')),\n",
    "                loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4987/4987 [==============================] - 5s 921us/step - loss: 0.6818 - accuracy: 0.1833\n",
      "0.18331651389598846\n"
     ]
    }
   ],
   "source": [
    "# Cell 14: Evaluate the export model on the test dataset\n",
    "loss, accuracy = export_model.evaluate(raw_test_ds)\n",
    "print(accuracy)\n",
    "\n",
    "#This is wack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Is_Toxic(text: str, threshold: int):\n",
    "    MessageList = [text]\n",
    "    predictions = export_model.predict(tf.constant(MessageList))\n",
    "\n",
    "    if predictions[0][0] >= threshold:\n",
    "        return (True, predictions[0][0])\n",
    "    else:\n",
    "        return (False, predictions[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Is_Toxic(\"I love you :)\", 0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_toxicity(predictions: list, examples: list):\n",
    "    plot_examples = range(len(examples))\n",
    "    toxicity_predictions = predictions.flatten()\n",
    "\n",
    "    # Create a bar graph\n",
    "    plt.bar(plot_examples, toxicity_predictions, color=['lightblue',])\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel('Examples')\n",
    "    plt.ylabel('Toxicity Probability')\n",
    "    plt.title('Toxicity Probability of Examples')\n",
    "\n",
    "    # Add x-axis labels for each example\n",
    "    plt.xticks(plot_examples, examples, rotation=45, ha='right')\n",
    "\n",
    "    # Display the graph\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: Save the export model\n",
    "will_save = input(\"Save the model? (y/n) \")\n",
    "if will_save == \"y\":\n",
    "    export_model.save('UMsave.keras')\n",
    "else:\n",
    "    print(\"Model not saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
